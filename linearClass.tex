\chapter{Linear Classifiers}
In this chapter we focus on 2-class classification problems.
In many cases the data is separable by a hyperplane in the feature space. 
In this case the discriminant function is:
\begin{equation*}
    g(x) = w^\text{T} x + w_0
\end{equation*}
$w \in \mathbb{R}^d$ is called the weight vector and $w_0 \in \mathbb{R}$ the threshold.
The weight vector is orthogonal to the hyperplane separating the feature space.

The distance of a point $p \in \mathbb{R}^d$ to the hyperplane is given by:
\begin{equation*}
    d(p) = \frac{\abs{g(p)}}{\norm{w}_2}
\end{equation*}

\section{Perceptron}
The perceptron is a single neuron with adaptable weights $w$ and threshold $\theta$ (called $w_0$ in the equation above). The class is given by $w^\text{T} x \geq \theta$.

\subsection{Perceptron Learning Algorithm}
We consider a two class problem with classes $\omega_1$ and $\omega_{-1}$, furthermore the classes are linearly separable. We define an extended weight vector $w^* = {(w_0, w_1, \ldots, w_d)}^\text{T} \in \mathbb{R}^{d+1}$ and extended input vector $x^* = {(1, x_1, \ldots, x_d)}^\text{T} \in \mathbb{R}^{d+1}$. This means there exist a $w^*$ with:
\begin{eqnarray*}
    w^{*\text{T}} x^* &>& 0\ \forall x \in \omega_1 \\
    w^{*\text{T}} x^* &<& 0\ \forall x \in \omega_{-1} \\
\end{eqnarray*}
The goal is to compute this vector. The perceptron learning algorithm:
\begin{algorithmic}
    \State choose an initial $w$
    \State set $\alpha > 0$
    \State $L \gets N$
    \While{$L > 0$}
        $L \gets 0$
        \For{$i=1$ to $N$}
            \If{$x_i \in \omega_1 \land w_\text{T} x_i < 0$}
                \State $w \gets w + \alpha x_i$
                \State $L \gets L + 1$
            \ElsIf{$x_i \in \omega_{-1} \land w_\text{T} x_i > 0$}
                \State $w \gets w - \alpha x_i$
                \State $L \gets L + 1$
            \EndIf
        \EndFor
    \EndWhile
\end{algorithmic}

The perceptron learning algorithm in Neural Network Formulation ($T_i \in \{-1, 1\}$ is the label of a class):
\begin{algorithmic}
    \State choose an initial $w$
    \State set $\alpha > 0$
    \State $L \gets N$
    \While{$L > 0$}
        $L \gets 0$
        \For{$i=1$ to $N$}
            \State $\Delta \gets T_i - \sign(w^\text{T} x_i)$
            \If{$\Delta \neq 0$}
                \State $w \gets w + \alpha \Delta x_i$
                \State $L \gets L + 1$
            \EndIf
        \EndFor
    \EndWhile
\end{algorithmic}

If the data is linearly separable the perceptron learning algorithm finds a solution (not the best) after a finite number of learning steps.

\todo[inline]{Proof?}

\section{Support Vector Machines}
The aim is to find a hyperplane separating the classes with maximal distance to both classes. 

Define $\min_i T_i (w^\text{T} x_i + w_0) = \delta$, to separate the classes $\delta > 0$. 
Rescaling by $\delta$ yields $w = \frac{1}{\delta} w$ and $w_0 = \frac{1}{\delta} w_0$.
To guarantee a maximum distance from both support vectors there should be a $x_i$ with $T_i = 1$ and $x_j$ with $T_j = -1$ which fullfill:
\begin{equation*}
    w^\text{T} x_i + w_0 = 1 \text{ and } w^\text{T} x_j + w_0 = -1
\end{equation*}
It follows, that:
\begin{equation*}
    w^\text{T} (x_i - x_j) = 2
\end{equation*}

The size of the margin is given by:
\begin{equation*}
    D(w) = \frac{w^\text{T} (x_i - x_j)}{\norm{w}_2} = \frac{2}{\norm{w}_2}
\end{equation*}

The numinator is constant, thus it is suffient to minimize the quadratic norm $\norm{w}_2^2$,
considering one constraint per data point (minimum margin and correct classification):
\begin{equation*}
    T_i (w^\text{T} x_i + w_0) \geq 1\ \forall i \in \{1, \ldots, N\}
\end{equation*}
This optimization can be solved using Langrange Optimization with the Karush-Kuhn-Tucker conditions.

\subsection{Soft separation}
If the data set is not linearly separable so-called slack variables need to be added to the constraints:
\begin{equation*}
    T_i (w^\text{T} x_i + w_0) \geq 1 - \delta_i\ \forall i \in \{1, \ldots, N\}
\end{equation*}

The optimization needs to consider the $\delta$s as well.

\section{Least Squares Methods}
We define a linear classifier
\begin{equation*}
    g(x) = w^\text{T} x + w_0
\end{equation*}
and a corresponding error function for classification:
\begin{equation*}
    E(w) = \sum_{i=1}^N {(T_i - g(x_i))}^2
\end{equation*}
If the data is ordered in a matrix (columns are variable, rows are observations) and the targets are encoded as vectors the problem can be formulated in matrix vector notation:
\begin{equation*}
    E(w) = \norm{T - X w }_2^2
\end{equation*}

The minimum is obviously achieved if $T = Xw$. This is the case if $X^{-1}$ exists, this can only be the case for $N=d$, so for most cases a minimium must be found differently.

\subsection{Minimization}
For a minimum of $E(w)$ it is necessary, that $\frac{\partial E(w)}{\partial w_k} = 0\ \forall k \in \{0, \ldots, d\}$:
\begin{equation*}
    \frac{\partial E(w)}{\partial w_k} = -2 \sum_{i=1}^N (T_i - w^\text{T} x_i) x_{ik} = 0
\end{equation*}
It follows:
\begin{equation*}
    \sum_{i=1}^N x_{ik} w^\text{T} x_i = \sum_{i=1}^N x_{ik} T_i 
\end{equation*}
In matrix vector notation:
\begin{equation*}
    X^\text{T} X w = X^\text{T} T
\end{equation*}
$X^\text{T}X$ is the correlation matrix of the (extended) data set.
If the inverse of the correlation matrix exists $w$ can be calculated:
\begin{equation*}
    w =  T
\end{equation*}
The matrix $X^+ = {(X^\text{T} X)}^{-1} X^\text{T}$ is the pseudo inverse of $X$.

If $X^\text{T}X$ is singular a pseudo inverse can be calculated:
\begin{equation*}
    X^+ = \lim_{\alpha \to \infty} {(X^\text{T} X + \alpha I)}^{-1} X^\text{T}
\end{equation*}

\section{Logistic Discrimination}
It is assumed that the log likelihood function is linear:
\begin{equation*}
    g_i(x) = \ln \left( \frac{P(\omega_i | x)}{P(\omega_M | x)} \right) = w^\text{T}_i x + w_{i0}
\end{equation*}
additionally the absolute probability must hold:
\begin{equation*}
    \sum_{k=1}^M P(\omega_k | x) = 1
\end{equation*}
The parameters $w$ and $w_0$ can then be calculated using the maximum likelihood approach.

\section{Linear Discriminant Analysis}
Aim: find a $w$ so that the data is easy to discriminate if projected onto $w$. For this the squared difference between the projected cluster centres is maximized. The cluster centres are given by:
\begin{eqnarray*}
    m_{-1} &=& \frac{1}{N_{-1}} \sum_{x_i \in \omega_{-1}} w^\text{T} x_i \\
    m_{1} &=& \frac{1}{N_{1}} \sum_{x_i \in \omega_{1}} w^\text{T} x_i
\end{eqnarray*}
The (squared) difference between the classes is given by:
\begin{equation*}
    \tilde{J}(w) = {(m_{-1} - m_1)}^2
\end{equation*}
This distance can be computed directly (with $c_{-1}$ and $c_1$ being the cluster centres):
\begin{equation*}
    \tilde{J}(w) = {(\T{w} (c_{-1} - c_1))}^2
\end{equation*}

The data is normalized with the two total variances $s_{-1}^2$ and $s_1^2$:
\begin{eqnarray*}
    s_{-1}^2 &=& \sum_{x_i \in \omega_{-1}} {(\T{w}x_i - m_{-1})}^2 \\
    s_{1}^2 &=& \sum_{x_i \in \omega_{1}} {(\T{w}x_i - m_{1})}^2 \\
\end{eqnarray*}

The function to optimize is then given by:
\begin{equation*}
    J(w) = \frac{{(m_{-1} - m_1)}^2}{s_{-1}^2 + s_1^2}
\end{equation*}
$J(w)$ can be reformulated by defining the between scatter matrix $S_B$ and the within scatter matrix $S_W$:
\begin{equation*}
    J(w) = \frac{\T{w} S_B w}{\T{w} S_W w}
\end{equation*}
To find the maximum it must hold that $\frac{\partial J(w)}{\partial w} = 0$. Thus:
\begin{equation*}
    (\T{w} S_W w) S_B w = (\T{w} S_B w) S_W w
\end{equation*}
Both $(\T{w} S_W w)$ and $(\T{w} S_B w)$ are positive real numbers.
Thus we can define a $\lambda \in \mathbb{R}^+$ as:
\begin{equation*}
    \lambda = \frac{\T{w} S_B w}{\T{w} S_W w}
\end{equation*}
As a result we get a generalized eigenvalue problem:
\begin{equation*}
    S_B w = \lambda S_W w
\end{equation*}
If $S_W^{-1}$ exist we can reformulate the problem as a regular eigenvalue problem:
\begin{equation*}
    S_W^{-1} S_B w = \lambda w
\end{equation*}
It follows that $w$ is in the direction $c_{-1} - c_1$. The threshold is derived using Bayes Decision Rule with the assumption of one gaussian distribution per class.
