\chapter{Linear Classifiers}
In this chapter we focus on 2-class classification problems.
In many cases the data is separable by a hyperplane in the feature space. 
In this case the discriminant function is:
\begin{equation*}
    g(x) = w^\text{T} x + w_0
\end{equation*}
$w \in \mathbb{R}^d$ is called the weight vector and $w_0 \in \mathbb{R}$ the threshold.
The weight vector is orthogonal to the hyperplane separating the feature space.

The distance of a point $p \in \mathbb{R}^d$ to the hyperplane is given by:
\begin{equation*}
    d(p) = \frac{\abs{g(p)}}{\norm{w}_2}
\end{equation*}

\section{Perceptron}
The perceptron is a single neuron with adaptable weights $w$ and threshold $\theta$ (called $w_0$ in the equation above). The class is given by $w^\text{T} x \geq \theta$.

\subsection{Perceptron Learning Algorithm}
We consider a two class problem with classes $\omega_1$ and $\omega_{-1}$, furthermore the classes are linearly separable. We define an extended weight vector $w^* = {(w_0, w_1, \ldots, w_d)}^\text{T} \in \mathbb{R}^{d+1}$ and extended input vector $x^* = {(1, x_1, \ldots, x_d)}^\text{T} \in \mathbb{R}^{d+1}$. This means there exist a $w^*$ with:
\begin{eqnarray*}
    w^{*\text{T}} x^* &>& 0\ \forall x \in \omega_1 \\
    w^{*\text{T}} x^* &<& 0\ \forall x \in \omega_{-1} \\
\end{eqnarray*}
The goal is to compute this vector. The perceptron learning algorithm:
\begin{algorithmic}
    \State choose an initial $w$
    \State set $\alpha > 0$
    \State $L \gets N$
    \While{$L > 0$}
        $L \gets 0$
        \For{$i=1$ to $N$}
            \If{$x_i \in \omega_1 \land w_\text{T} x_i < 0$}
                \State $w \gets w + \alpha x_i$
                \State $L \gets L + 1$
            \ElsIf{$x_i \in \omega_{-1} \land w_\text{T} x_i > 0$}
                \State $w \gets w - \alpha x_i$
                \State $L \gets L + 1$
            \EndIf
        \EndFor
    \EndWhile
\end{algorithmic}

The perceptron learning algorithm in Neural Network Formulation ($T_i \in \{-1, 1\}$ is the label of a class):
\begin{algorithmic}
    \State choose an initial $w$
    \State set $\alpha > 0$
    \State $L \gets N$
    \While{$L > 0$}
        $L \gets 0$
        \For{$i=1$ to $N$}
            \State $\Delta \gets T_i - \sign(w^\text{T} x_i)$
            \If{$\Delta \neq 0$}
                \State $w \gets w + \alpha \Delta x_i$
                \State $L \gets L + 1$
            \EndIf
        \EndFor
    \EndWhile
\end{algorithmic}

If the data is linearly separable the perceptron learning algorithm finds a solution (not the best) after a finite number of learning steps.

\todo[inline]{Proof?}

\section{Support Vector Machines}
The aim is to find a hyperplane separating the classes with maximal distance to both classes. 

Define $\min_i T_i (w^\text{T} x_i + w_0) = \delta$, to separate the classes $\delta > 0$. 
Rescaling by $\delta$ yields $w = \frac{1}{\delta} w$ and $w_0 = \frac{1}{\delta} w_0$.
To guarantee a maximum distance from both support vectors there should be a $x_i$ with $T_i = 1$ and $x_j$ with $T_j = -1$ which fullfill:
\begin{equation*}
    w^\text{T} x_i + w_0 = 1 \text{ and } w^\text{T} x_j + w_0 = -1
\end{equation*}
It follows, that:
\begin{equation*}
    w^\text{T} (x_i - x_j) = 2
\end{equation*}

The size of the margin is given by:
\begin{equation*}
    D(w) = \frac{w^\text{T} (x_i - x_j)}{\norm{w}_2} = \frac{2}{\norm{w}_2}
\end{equation*}

The numinator is constant, thus it is suffient to minimize the quadratic norm $\norm{w}_2^2$,
considering one constraint per data point (minimum margin and correct classification):
\begin{equation*}
    T_i (w^\text{T} x_i + w_0) \geq 1\ \forall i \in \{1, \ldots, N\}
\end{equation*}
This optimization can be solved using Langrange Optimization with the Karush-Kuhn-Tucker conditions.

\subsection{Soft separation}
If the data set is not linearly separable so-called slack variables need to be added to the constraints:
\begin{equation*}
    T_i (w^\text{T} x_i + w_0) \geq 1 - \delta_i\ \forall i \in \{1, \ldots, N\}
\end{equation*}

The optimization needs to consider the $\delta$s as well.

\section{Least Squares Methods}
We define a linear classifier
\begin{equation*}
    g(x) = w^\text{T} x + w_0
\end{equation*}
and a corresponding error function for classification:
\begin{equation*}
    E(w) = \sum_{i=1}^N {(T_i - g(x_i))}^2
\end{equation*}
If the data is ordered in a matrix (columns are variable, rows are observations) and the targets are encoded as vectors the problem can be formulated in matrix vector notation:
\begin{equation*}
    E(w) = \norm{T - X w }_2^2
\end{equation*}

The minimum is obviously achieved if $T = Xw$. This is the case if $X^{-1}$ exists, this can only be the case for $N=d$, so for most cases a minimium must be found differently.

\subsection{Minimization}
For a minimum of $E(w)$ it is necessary, that $\frac{\partial E(w)}{\partial w_k} = 0\ \forall k \in \{0, \ldots, d\}$:
\begin{equation*}
    \frac{\partial E(w)}{\partial w_k} = -2 \sum_{i=1}^N (T_i - w^\text{T} x_i) x_{ik} = 0
\end{equation*}
It follows:
\begin{equation*}
    \sum_{i=1}^N x_{ik} w^\text{T} x_i = \sum_{i=1}^N x_{ik} T_i 
\end{equation*}
In matrix vector notation:
\begin{equation*}
    X^\text{T} X w = X^\text{T} T
\end{equation*}
